{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The purpose of this utility is to make general purpose data transformations and gathered in one single place.\n",
    "\n",
    "## Porposed utilities:\n",
    "\n",
    "### Dataset shrink\n",
    "\n",
    "Takes an input dataset and create subsets with the information presented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import floor\n",
    "import os\n",
    "\n",
    "data_path = \"../data\"\n",
    "processed_data_path = \"../processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_files(file_name: str, partition_list, partition_mode: str=\"all\", data_path: str=\"../data\"):\n",
    "    \"\"\"\n",
    "    This function reads a csv file and creates sub files based on the partition list and partition mode\n",
    "    partition_list: list of partition values. Each partition should be between 0 and 1\n",
    "    if partition mode is \"one\", then it creates one file for each partition value\n",
    "    if partition mode is \"all\", then it creates 1/partition value files plus one file with the remaining data\n",
    "        example: if partition_list = [0.2, 0.3] and partition_mode = \"all\", then it creates 9 files\n",
    "        1. 20% of the data will be created in each file (5 files)\n",
    "            The naming convention will be:\n",
    "                file_name_2000_1_of_5.csv\n",
    "                file_name_2000_2_2_of_5.csv\n",
    "                file_name_2000_2_3_of_5.csv\n",
    "                file_name_2000_2_4_of_5.csv\n",
    "                file_name_2000_2_5_of_5.csv\n",
    "        2. 45% of the data will be created in each file + 10% in a remainded file (4 files) \n",
    "            The naming convention will be:\n",
    "                file_name_4500_1_of_3.csv\n",
    "                file_name_4500_2_of_3.csv\n",
    "                file_name_4500_3_of_3.csv\n",
    "    \"\"\"\n",
    "    print(f\"Creating sub files for {file_name}, partition_list: {partition_list}, partition_mode: {partition_mode}\")\n",
    "    assert file_name is not None and file_name != \"\", \"File name should not be empty\"\n",
    "    assert \"/\" not in file_name and \"\\\\\" not in file_name, \"File name should not contain path\"\n",
    "    file_split = file_name.split(\".\")\n",
    "    assert len(file_split) == 2, \"File name should have one extension only\"\n",
    "    for i in partition_list:\n",
    "        assert i >= 0 and i <= 1, \"Partition value should be between 0 and 1\"\n",
    "    assert partition_mode in [\"all\", \"one\"], \"Partition mode should be either all or one\"\n",
    "\n",
    "    df = pd.read_csv(f\"{data_path}/{file_name}\")\n",
    "    if partition_mode == \"one\":\n",
    "        for i in partition_list:\n",
    "            partition_str = f\"{int(i*10000):05}\"\n",
    "            df1 = df.sample(frac=i)\n",
    "            df1.to_csv(f\"{processed_data_path}/{file_split[0]}_{partition_str}.{file_split[1]}\", index=False)\n",
    "    elif partition_mode == \"all\":\n",
    "        for i in partition_list:\n",
    "            partition_str = f\"{int(i*10000):05}\"\n",
    "            df = df.sample(frac=1).reset_index(drop=True)\n",
    "            n_rows = floor(i * len(df))\n",
    "            num_files = floor(1/i) if floor(1/i)*n_rows == len(df) else floor(1/i) + 1\n",
    "            for j in range(floor(1/i)):\n",
    "                df1 = df.iloc[j*n_rows:(j+1)*n_rows]\n",
    "                df1.to_csv(f\"{processed_data_path}/{file_split[0]}_{partition_str}_{j+1}_of_{num_files}.{file_split[1]}\", index=False)\n",
    "            if (floor(1/i))*n_rows < len(df):\n",
    "                df1 = df.iloc[(floor(1/i))*n_rows:]\n",
    "                df1.to_csv(f\"{processed_data_path}/{file_split[0]}_{partition_str}_{num_files}_of_{num_files}.{file_split[1]}\", index=False)\n",
    "\n",
    "\n",
    "def kfold_split(file_name: str, k: int, data_path:str=\"../data\"):\n",
    "    \"\"\"\n",
    "    This function reads a csv file and creates k files with the same number of rows\n",
    "        File format:\n",
    "            file_name_1_of_5.csv\n",
    "    \"\"\"\n",
    "    print(f\"Creating kfold split for {file_name}, k: {k}\")\n",
    "    assert file_name is not None and file_name != \"\", \"File name should not be empty\"\n",
    "    assert \"/\" not in file_name and \"\\\\\" not in file_name, \"File name should not contain path\"\n",
    "    file_split = file_name.split(\".\")\n",
    "    assert len(file_split) == 2, \"File name should have one extension only\"\n",
    "    assert k > 1, \"k should be greater than 1\"\n",
    "\n",
    "    df = pd.read_csv(f\"{data_path}/{file_name}\").sample(frac=1)\n",
    "    df[\"TEMP_KFOLD\"] = ((df.index) % k)\n",
    "    for i in range(k):\n",
    "        df1 = df[df[\"TEMP_KFOLD\"] == i]\n",
    "        df1 = df1.drop(columns=[\"TEMP_KFOLD\"])\n",
    "        df1.to_csv(f\"{processed_data_path}/{file_split[0]}_{i+1}_of_{k}.{file_split[1]}\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test cases for the functions above.\n",
    "train_file = \"sales_train.csv\"\n",
    "test_file = \"sales_test.csv\"\n",
    "\n",
    "\n",
    "def test_create_sub_files_one():\n",
    "    create_sub_files(test_file, [0.2, 0.3], \"one\", data_path)\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_02000.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_03000.csv\")\n",
    "\n",
    "def test_create_sub_files_all():\n",
    "    create_sub_files(test_file, [0.45], \"all\", data_path)\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_04500_1_of_3.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_04500_2_of_3.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_04500_3_of_3.csv\")\n",
    "\n",
    "def test_kfold_split():\n",
    "    kfold_split(test_file, 5, data_path)\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_1_of_5.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_2_of_5.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_3_of_5.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_4_of_5.csv\")\n",
    "    assert os.path.exists(f\"{processed_data_path}/sales_test_5_of_5.csv\")\n",
    "\n",
    "def test_clean_up():\n",
    "    os.remove(f\"{processed_data_path}/sales_test_02000.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_03000.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_04500_1_of_3.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_04500_2_of_3.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_04500_3_of_3.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_1_of_5.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_2_of_5.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_3_of_5.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_4_of_5.csv\")\n",
    "    os.remove(f\"{processed_data_path}/sales_test_5_of_5.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_handler = [test_create_sub_files_one, test_create_sub_files_all, test_kfold_split]\n",
    "for test_case in test_handler:\n",
    "    test_case()\n",
    "    print(f\"{test_case.__name__} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cases clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for kaggle rohlik competition\n",
    "create_sub_files(\"sales_train.csv\", [0.01, 0.1, 0.3], \"one\")\n",
    "create_sub_files(\"sales_test.csv\", [0.01, 0.1, 0.3], \"one\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
